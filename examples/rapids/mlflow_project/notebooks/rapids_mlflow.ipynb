{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import subprocess\n",
    "import sys\n",
    "import threading\n",
    "from queue import Queue, Empty\n",
    "\n",
    "from functools import partial\n",
    "\n",
    "import mlflow\n",
    "import mlflow.sklearn\n",
    "\n",
    "from cuml.metrics.accuracy import accuracy_score\n",
    "from cuml.preprocessing.model_selection import train_test_split\n",
    "from cuml.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pull sample airline data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget -N https://rapidsai-cloud-ml-sample-data.s3-us-west-2.amazonaws.com/airline_small.parquet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define data loader, using cuDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(fpath):\n",
    "    \"\"\"\n",
    "    Simple helper function for loading data to be used by CPU/GPU models.\n",
    "\n",
    "    :param fpath: Path to the data to be ingested\n",
    "    :return: DataFrame wrapping the data at [fpath]. Data will be in either a Pandas or RAPIDS (cuDF) DataFrame\n",
    "    \"\"\"\n",
    "    import cudf\n",
    "\n",
    "    df = cudf.read_parquet(fpath)\n",
    "    X = df.drop([\"ArrDelayBinary\"], axis=1)\n",
    "    y = df[\"ArrDelayBinary\"].astype('int32')\n",
    "    \n",
    "    return train_test_split(X, y, test_size=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define our training routine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(fpath, max_detph, max_features, n_estimators):\n",
    "    \"\"\"\n",
    "    :param fpath: Path or URL for the training data used with the model.\n",
    "    :max_detph: int Max tree depth\n",
    "    :max_features: float percentage of features to use in classification\n",
    "    :n_estimators: int number of trees to create\n",
    "    :return: Trained Model\n",
    "    \"\"\"\n",
    "    X_train, X_test, y_train, y_test = load_data(fpath)\n",
    "    mod = RandomForestClassifier(max_depth=max_depth, max_features=max_features, n_estimators=n_estimators)\n",
    "    acc_scorer = accuracy_score\n",
    "\n",
    "    mod.fit(X_train, y_train)\n",
    "    preds = mod.predict(X_test)\n",
    "    acc = acc_scorer(y_test, preds)\n",
    "\n",
    "    mlparams = {\"max_depth\": str(max_depth),\n",
    "                \"max_features\": str(max_features),\n",
    "                \"n_estimators\": str(n_estimators),\n",
    "                }\n",
    "    mlflow.log_params(mlparams)\n",
    "\n",
    "    mlmetrics = {\"accuracy\": acc}\n",
    "    mlflow.log_metrics(mlmetrics)\n",
    "\n",
    "    return mod"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implement our MLFlow training loop, and save our best model to the tracking server."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conda_env = f'conda.yaml'\n",
    "fpath     = f'airline_small.parquet'\n",
    "\n",
    "max_depth = 10\n",
    "max_features = 0.75\n",
    "n_estimators = 500\n",
    "\n",
    "with mlflow.start_run():\n",
    "    mlflow.set_tag(\"mlflow.runName\", \"RAPIDS-MLFlow\")\n",
    "    \n",
    "    model = train(fpath, max_depth, max_features, n_estimators)\n",
    "    \n",
    "    mlflow.sklearn.log_model(model,\n",
    "                             artifact_path=\"RAPIDS-MLFlow\",\n",
    "                             conda_env='conda.yaml')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Serving\n",
    "Basic file based storage does not support model registry features, so we will need to manually identify our saved model for serving. When running with a database backed MLFLow Tracking sever, model identification can be done using the [MlFlow Tracking Client](https://www.mlflow.org/docs/latest/tracking.html).\n",
    "\n",
    "1. Run `mlflow ui` and connect to `localhost:5000`\n",
    "1. Identify the model we just created, labeled 'RAPIDS-MLFlow'\n",
    "    1. Select the model\n",
    "    1. Select Artifacts\n",
    "    1. Copy 'Full Path'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_path = 'file:///[PATH TO LOCAL MLFLOW RUN]'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper to track our server output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def queue_descriptor_output(out, queue):\n",
    "    for line in iter(out.readline, b''):\n",
    "        queue.put(line)\n",
    "    out.close()\n",
    "\n",
    "def follow_subprocess(cmd, timeout=1000, line_timeout=60.00):\n",
    "    p = subprocess.Popen(cmd, stdout=subprocess.PIPE, stderr=subprocess.STDOUT)\n",
    "    q = Queue()\n",
    "    t = threading.Thread(target=queue_descriptor_output, args=(p.stdout, q))\n",
    "    t.daemon = True\n",
    "    t.start()\n",
    "\n",
    "    elapsed = 0\n",
    "    line_elapsed = 0\n",
    "    last_line_time = time.perf_counter()\n",
    "    while (p.poll() is None and elapsed < timeout and line_elapsed < line_timeout):\n",
    "        try:\n",
    "            time.sleep(2)\n",
    "            elapsed += 2\n",
    "            while (True):\n",
    "                line = q.get(timeout=0.1)\n",
    "                line_elapsed = 0\n",
    "                last_line_time = time.perf_counter()\n",
    "                sys.stdout.write(line.decode())\n",
    "\n",
    "        except Empty:\n",
    "            line_elapsed = (time.perf_counter() - last_line_time)\n",
    "        except KeyboardInterrupt:\n",
    "            sys.stderr.write(\"\\nCaught ctrl+c, killing subprocess ({})\\n\".format(' '.join(cmd)))\n",
    "            p.kill()\n",
    "            raise\n",
    "\n",
    "    try:\n",
    "        p.kill()\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    t.join(2)\n",
    "\n",
    "    ## Drain any remaining text\n",
    "    try:\n",
    "        while (True):\n",
    "            line = q.get(timeout=0.1)\n",
    "            sys.stdout.write(line)\n",
    "\n",
    "    except Empty:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Begin serving our trained model using MLFlow\n",
    "**Note:** The serving thread will continue to run after cell execution. Select the cell and click 'interrupt the kernel' to stop it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "port = 55755\n",
    "host = 'localhost'\n",
    "\n",
    "command = f\"mlflow models serve --no-conda -m {full_path} -p {port} -h {host}\".split()\n",
    "kwargs = { \"cmd\": command, \"timeout\":float('Inf'), \"line_timeout\": float('Inf') }\n",
    "\n",
    "threading.Thread(target=follow_subprocess, kwargs=kwargs).start()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make requests against the deployed model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import requests\n",
    "\n",
    "headers = {\n",
    "    \"Content-Type\": \"application/json\",\n",
    "    \"format\": \"pandas-split\"\n",
    "}\n",
    "\n",
    "data = { \n",
    "    \"columns\": [\"Year\", \"Month\", \"DayofMonth\", \"DayofWeek\", \"CRSDepTime\", \"CRSArrTime\", \"UniqueCarrier\", \"FlightNum\", \"ActualElapsedTime\", \"Origin\", \"Dest\", \"Distance\", \"Diverted\"],\n",
    "    \"data\": [[1987, 10, 1, 4, 1, 556, 0, 190, 247, 202, 162, 1846, 0]]\n",
    "}\n",
    "\n",
    "## Pause to let server start\n",
    "time.sleep(5)\n",
    "\n",
    "while (True):\n",
    "    try:\n",
    "        resp = requests.post(url=f\"http://{host}:{port}/invocations\", data=json.dumps(data), headers=headers)\n",
    "        print(f'Classification: {\"ON-Time\" if resp.text == \"[0.0]\" else \"LATE\"}')\n",
    "        break\n",
    "    except Exception as e:\n",
    "        errmsg = f\"Caught exception attempting to call model endpoint: {e}\"\n",
    "        print(f\"{errmsg}\", end='')\n",
    "        print(f\"Sleeping\")\n",
    "        time.sleep(20)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlflow",
   "language": "python",
   "name": "mlflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
